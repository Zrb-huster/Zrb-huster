{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4dc17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备：cuda:0\n",
      "\n",
      "=== ViT前向传播测试结果 ===\n",
      "输入图像形状：torch.Size([2, 3, 32, 32])（预期：[2,3,32,32]）\n",
      "分类预测形状：torch.Size([2, 10])（预期：[2,10]）\n",
      "注意力权重形状：torch.Size([12, 2, 65, 65])（预期：[12,2,65,65]）\n",
      "预测类别：tensor([3, 8], device='cuda:0')（真实类别：tensor([3, 8], device='cuda:0')）\n",
      "\n",
      "ViT前向模型实现完成，维度匹配预期，可正常用于后续视觉任务！\n"
     ]
    }
   ],
   "source": [
    "# 导入依赖库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Transformer Encoder基础组件（用于复用至ViT）\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention层：实现注意力分数计算、缩放与掩码功能\"\"\"\n",
    "    def __init__(self, dim_q, dim_k, dim_v):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.linear_q = nn.Linear(dim_q, dim_k, bias=False)\n",
    "        self.linear_k = nn.Linear(dim_q, dim_k, bias=False)\n",
    "        self.linear_v = nn.Linear(dim_q, dim_v, bias=False)\n",
    "        self._norm_fact = 1 / math.sqrt(dim_k)  # 缩放因子，避免分数值过大导致SoftMax梯度消失\n",
    "        self.dim_q = dim_q\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, dim_q = x.shape\n",
    "        assert dim_q == self.dim_q, f\"输入维度{dim_q}与初始化dim_q{self.dim_q}不匹配\"\n",
    "        \n",
    "        # 生成Q、K、V\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        \n",
    "        # 计算注意力分数并缩放\n",
    "        attn_scores = torch.bmm(q, k.transpose(1, 2)) * self._norm_fact\n",
    "        # 应用掩码（屏蔽无效位置）\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 注意力权重归一化与上下文向量计算\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        att = torch.bmm(attn_weights, v)\n",
    "        return att, attn_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention层：将输入拆分为多子空间并行计算注意力，提升模型表达能力\"\"\"\n",
    "    def __init__(self, dim_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert dim_model % num_heads == 0, f\"输入维度{dim_model}需能被头数{num_heads}整除\"\n",
    "        \n",
    "        self.dim_model = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim_model // num_heads  # 每个注意力头的维度\n",
    "        \n",
    "        # Q、K、V线性映射层\n",
    "        self.linear_q = nn.Linear(dim_model, dim_model, bias=False)\n",
    "        self.linear_k = nn.Linear(dim_model, dim_model, bias=False)\n",
    "        self.linear_v = nn.Linear(dim_model, dim_model, bias=False)\n",
    "        # 复用Scaled Dot-Product Attention\n",
    "        self.self_attn = SelfAttention(dim_q=self.head_dim, dim_k=self.head_dim, dim_v=self.head_dim)\n",
    "        # 多头结果拼接后的线性变换层\n",
    "        self.linear_out = nn.Linear(dim_model, dim_model, bias=False)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"将输入拆分为多个注意力头，确保张量内存连续以适配后续操作\"\"\"\n",
    "        batch_size, seq_len, dim_model = x.shape\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def _concat_heads(self, x):\n",
    "        \"\"\"将多个注意力头的输出拼接为完整维度\"\"\"\n",
    "        batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, num_heads * head_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, dim_model = x.shape\n",
    "        assert dim_model == self.dim_model, f\"输入维度{dim_model}与初始化dim_model{self.dim_model}不匹配\"\n",
    "        \n",
    "        # Q、K、V线性映射\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        \n",
    "        # 拆分多头并计算注意力\n",
    "        q_split = self._split_heads(q)\n",
    "        k_split = self._split_heads(k)\n",
    "        v_split = self._split_heads(v)\n",
    "        \n",
    "        # 展平批次与头维度，适配SelfAttention输入\n",
    "        q_reshaped = q_split.view(-1, seq_len, self.head_dim)\n",
    "        k_reshaped = k_split.view(-1, seq_len, self.head_dim)\n",
    "        v_reshaped = v_split.view(-1, seq_len, self.head_dim)\n",
    "        mask_reshaped = mask.repeat(self.num_heads, 1, 1) if mask is not None else None\n",
    "        \n",
    "        att_split, att_weights_split = self.self_attn(q_reshaped, mask=mask_reshaped)\n",
    "        \n",
    "        # 拼接多头结果并线性变换\n",
    "        att_reshaped = att_split.view(batch_size, self.num_heads, seq_len, self.head_dim)\n",
    "        att_concat = self._concat_heads(att_reshaped)\n",
    "        out = self.linear_out(att_concat)\n",
    "        \n",
    "        # 计算所有头的平均注意力权重\n",
    "        att_weights = att_weights_split.view(batch_size, self.num_heads, seq_len, seq_len).mean(dim=1)\n",
    "        return out, att_weights\n",
    "\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"Add&Norm层：实现残差连接与层归一化，稳定模型训练\"\"\"\n",
    "    def __init__(self, dim_model, eps=1e-6):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(dim_model, eps=eps)  # 层归一化，避免内部协变量偏移\n",
    "        self.residual_weight = nn.Parameter(torch.ones(1))  # 残差连接权重，提升灵活性\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        # 残差相加（Add）\n",
    "        add_out = x + self.residual_weight * residual\n",
    "        # 层归一化（Norm）\n",
    "        out = self.norm(add_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"前馈网络（FFN）：对注意力输出做非线性变换，增强模型表达能力\"\"\"\n",
    "    def __init__(self, dim_model, hidden_dim=3072, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_model, hidden_dim, bias=True)\n",
    "        self.gelu = nn.GELU()  # GELU激活函数，相比ReLU更易捕捉非线性特征\n",
    "        self.dropout = nn.Dropout(dropout)  #  dropout正则化，防止过拟合\n",
    "        self.linear2 = nn.Linear(hidden_dim, dim_model, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear2(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"单个Transformer Encoder层：Multi-Head Attention → Add&Norm → FFN → Add&Norm\"\"\"\n",
    "    def __init__(self, dim_model, num_heads, hidden_dim=3072, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attn = MultiHeadAttention(dim_model=dim_model, num_heads=num_heads)\n",
    "        self.add_norm1 = AddNorm(dim_model=dim_model)\n",
    "        self.feed_forward = FeedForward(dim_model=dim_model, hidden_dim=hidden_dim, dropout=dropout)\n",
    "        self.add_norm2 = AddNorm(dim_model=dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 多头注意力计算\n",
    "        att_out, att_weights = self.multi_head_attn(x, mask=mask)\n",
    "        att_out = self.dropout(att_out)\n",
    "        # 第一次Add&Norm\n",
    "        add_norm1_out = self.add_norm1(att_out, residual=x)\n",
    "        \n",
    "        # 前馈网络计算\n",
    "        ff_out = self.feed_forward(add_norm1_out)\n",
    "        ff_out = self.dropout(ff_out)\n",
    "        # 第二次Add&Norm\n",
    "        out = self.add_norm2(ff_out, residual=add_norm1_out)\n",
    "        \n",
    "        return out, att_weights\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"完整Transformer Encoder：堆叠多个EncoderLayer，实现深度特征编码\"\"\"\n",
    "    def __init__(self, dim_model=768, num_heads=12, num_layers=12, hidden_dim=3072, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # 堆叠num_layers个EncoderLayer\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                dim_model=dim_model,\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, dim_model = x.shape\n",
    "        assert dim_model == self.dim_model, f\"输入维度{dim_model}与初始化dim_model{self.dim_model}不匹配\"\n",
    "        \n",
    "        out = x\n",
    "        all_att_weights = []\n",
    "        # 逐层传递计算\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            out, att_weights = encoder_layer(out, mask=mask)\n",
    "            all_att_weights.append(att_weights)\n",
    "        \n",
    "        # 收集所有层的注意力权重\n",
    "        all_att_weights = torch.stack(all_att_weights, dim=0)\n",
    "        return out, all_att_weights\n",
    "\n",
    "\n",
    "# ViT核心组件：Patch Embedding（图像分块与线性投影）\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"将图像拆分为固定尺寸的块，并通过线性投影转换为指定维度的特征序列\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, dim_model=768):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        # 确保图像尺寸能被块尺寸整除，实现无重叠分块\n",
    "        assert img_size % patch_size == 0, f\"图像尺寸{img_size}需能被块尺寸{patch_size}整除\"\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2  # 图像拆分后的总块数\n",
    "        # 用卷积层实现“分块+线性投影”（kernel=patch_size，stride=patch_size）\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=dim_model,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入x：[batch_size, in_channels, img_size, img_size]（如CIFAR-10：[B,3,32,32]）\n",
    "        x = self.proj(x)  # 卷积分块：[B, dim_model, img_size/patch_size, img_size/patch_size]\n",
    "        patch_emb = x.flatten(2)  # 展平块维度：[B, dim_model, num_patches]\n",
    "        patch_emb = patch_emb.transpose(1, 2)  # 调整为序列格式：[B, num_patches, dim_model]\n",
    "        return patch_emb\n",
    "\n",
    "\n",
    "# 完整Vision Transformer（ViT）前向模型\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer：将Transformer架构迁移至视觉任务，实现图像分类前向传播\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, dim_model=768, \n",
    "                 num_heads=12, num_layers=12, hidden_dim=3072, num_classes=10, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        # 1. 图像块嵌入（Patch Embedding）\n",
    "        self.patch_emb = PatchEmbedding(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            dim_model=dim_model\n",
    "        )\n",
    "        self.num_patches = self.patch_emb.num_patches  # 图像拆分后的总块数\n",
    "        \n",
    "        # 2. 分类Token（[class] Token）：用于聚合全局特征，生成分类结果\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, dim_model))  # 可学习参数\n",
    "        # 3. 位置嵌入（Positional Embedding）：注入图像块的位置信息\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, self.num_patches + 1, dim_model))  # +1为分类Token\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 4. 复用Transformer Encoder：对块特征序列做深度编码\n",
    "        self.encoder = TransformerEncoder(\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 5. 分类头：基于分类Token的输出生成最终分类结果\n",
    "        self.norm = nn.LayerNorm(dim_model)  # 层归一化，稳定输出分布\n",
    "        self.fc = nn.Linear(dim_model, num_classes)  # 线性映射到类别数\n",
    "\n",
    "        # 参数初始化：采用Xavier均匀初始化，避免初始梯度异常\n",
    "        nn.init.xavier_uniform_(self.class_token)\n",
    "        nn.init.xavier_uniform_(self.pos_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入x：[batch_size, in_channels, img_size, img_size]（如CIFAR-10：[B,3,32,32]）\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 步骤1：图像分块与线性投影\n",
    "        patch_emb = self.patch_emb(x)  # [B, num_patches, dim_model]\n",
    "        \n",
    "        # 步骤2：添加分类Token（扩展到批次大小后与块特征拼接）\n",
    "        class_token = self.class_token.expand(batch_size, -1, -1)  # [B,1,dim_model]\n",
    "        seq = torch.cat([class_token, patch_emb], dim=1)  # [B, num_patches+1, dim_model]\n",
    "        \n",
    "        # 步骤3：添加位置嵌入并应用dropout\n",
    "        seq = seq + self.pos_emb  # 注入位置信息\n",
    "        seq = self.dropout(seq)\n",
    "        \n",
    "        # 步骤4：Transformer Encoder编码\n",
    "        encoder_out, all_att_weights = self.encoder(seq)  # [B, num_patches+1, dim_model]\n",
    "        \n",
    "        # 步骤5：提取分类Token的输出，生成分类预测\n",
    "        class_token_out = encoder_out[:, 0, :]  # 分类Token对应输出（第0位）\n",
    "        class_token_out = self.norm(class_token_out)  # 层归一化\n",
    "        logits = self.fc(class_token_out)  # [B, num_classes]（分类预测结果）\n",
    "        \n",
    "        return logits, all_att_weights\n",
    "\n",
    "\n",
    "# CIFAR-10数据集加载（适配ViT输入需求）\n",
    "def load_cifar10(batch_size=64):\n",
    "    \"\"\"加载CIFAR-10数据集，包含训练集数据增强与验证集标准化预处理\"\"\"\n",
    "    # 训练集预处理：随机裁剪+水平翻转（数据增强）+标准化\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))  # CIFAR-10统计参数\n",
    "    ])\n",
    "    # 验证集预处理：仅标准化（避免数据分布偏移，确保评估客观）\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    # 加载数据集（自动下载至指定路径）\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=train_transform\n",
    "    )\n",
    "    val_dataset = datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=val_transform\n",
    "    )\n",
    "\n",
    "    # 构建数据加载器（多线程加载，提升数据读取效率）\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# ViT前向模型测试（验证模型完整性与维度正确性）\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 测试参数配置（适配CIFAR-10与GPU环境）\n",
    "    batch_size = 2\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备：{device}\")\n",
    "\n",
    "    # 2. 加载CIFAR-10验证集（仅用于前向测试，无需训练）\n",
    "    _, val_loader = load_cifar10(batch_size=batch_size)\n",
    "    val_iter = iter(val_loader)\n",
    "    imgs, labels = next(val_iter)  # 获取1批次测试数据\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "    # 3. 初始化ViT模型（采用ViT-Base简化配置，适配GPU显存）\n",
    "    vit_model = VisionTransformer(\n",
    "        img_size=32,    # CIFAR-10图像尺寸\n",
    "        patch_size=4,   # 4×4图像块（无重叠分块）\n",
    "        num_classes=10, # CIFAR-10类别数\n",
    "        dim_model=768,  # 特征维度（ViT-Base标准值）\n",
    "        num_heads=12,   # 注意力头数（ViT-Base标准值）\n",
    "        num_layers=12   # Encoder层数（ViT-Base标准值）\n",
    "    )\n",
    "    vit_model = vit_model.to(device)\n",
    "\n",
    "    # 4. 执行前向传播（禁用梯度计算，节省内存）\n",
    "    vit_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, all_att_weights = vit_model(imgs)\n",
    "\n",
    "    # 5. 验证输出维度与结果合理性\n",
    "    print(f\"\\n=== ViT前向传播测试结果 ===\")\n",
    "    print(f\"输入图像形状：{imgs.shape}（预期：[{batch_size},3,32,32]）\")\n",
    "    print(f\"分类预测形状：{logits.shape}（预期：[{batch_size},10]）\")\n",
    "    print(f\"注意力权重形状：{all_att_weights.shape}（预期：[12,{batch_size},{65},{65}]）\")\n",
    "    print(f\"预测类别：{torch.argmax(logits, dim=1)}（真实类别：{labels}）\")\n",
    "    print(\"\\nViT前向模型实现完成，维度匹配预期，可正常用于后续视觉任务！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
