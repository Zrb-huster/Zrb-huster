{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19262414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Transformer Encoder测试结果 ===\n",
      "输入随机矩阵形状：torch.Size([2, 16, 512])\n",
      "Encoder输出形状：torch.Size([2, 16, 512])（预期：[2, 16, 512]）\n",
      "注意力权重形状：torch.Size([6, 2, 16, 16])（预期：[6, 2, 16, 16]）\n",
      "掩码有效性：第1层第1批次第1序列后8位权重均值 = 0.000000（接近0为正常）\n",
      "\n",
      "Transformer Encoder实现完成，可正常用于后续任务！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "# 复用已实现的注意力相关组件\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-product Attention层：计算注意力分数并缩放，支持可选掩码\"\"\"\n",
    "    def __init__(self, dim_q, dim_k, dim_v):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.linear_q = nn.Linear(dim_q, dim_k, bias=False)\n",
    "        self.linear_k = nn.Linear(dim_q, dim_k, bias=False)\n",
    "        self.linear_v = nn.Linear(dim_q, dim_v, bias=False)\n",
    "        self._norm_fact = 1 / math.sqrt(dim_k)  # 缩放因子，避免分数值过大\n",
    "        self.dim_q = dim_q\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, dim_q = x.shape\n",
    "        assert dim_q == self.dim_q, f\"输入维度{dim_q}与初始化dim_q{self.dim_q}不匹配\"\n",
    "        \n",
    "        # 生成Q、K、V向量\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        \n",
    "        # 计算注意力分数并缩放\n",
    "        attn_scores = torch.bmm(q, k.transpose(1, 2)) * self._norm_fact\n",
    "        # 应用掩码（屏蔽无效位置）\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 注意力权重归一化与上下文向量计算\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        att = torch.bmm(attn_weights, v)\n",
    "        return att, attn_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention层：将输入拆分为多子空间并行计算注意力，提升表达能力\"\"\"\n",
    "    def __init__(self, dim_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert dim_model % num_heads == 0, f\"输入维度{dim_model}需能被头数{num_heads}整除\"\n",
    "        \n",
    "        self.dim_model = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim_model // num_heads  # 单个注意力头的维度\n",
    "        \n",
    "        # Q、K、V线性映射层\n",
    "        self.linear_q = nn.Linear(dim_model, dim_model, bias=False)\n",
    "        self.linear_k = nn.Linear(dim_model, dim_model, bias=False)\n",
    "        self.linear_v = nn.Linear(dim_model, dim_model, bias=False)\n",
    "        # 复用Scaled Dot-product Attention\n",
    "        self.self_attn = SelfAttention(dim_q=self.head_dim, dim_k=self.head_dim, dim_v=self.head_dim)\n",
    "        # 多头结果拼接后的线性变换层\n",
    "        self.linear_out = nn.Linear(dim_model, dim_model, bias=False)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"将输入拆分为多个注意力头，确保张量内存连续\"\"\"\n",
    "        batch_size, seq_len, dim_model = x.shape\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def _concat_heads(self, x):\n",
    "        \"\"\"将多个注意力头的输出拼接为完整维度\"\"\"\n",
    "        batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, num_heads * head_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, dim_model = x.shape\n",
    "        assert dim_model == self.dim_model, f\"输入维度{dim_model}与初始化dim_model{self.dim_model}不匹配\"\n",
    "        \n",
    "        # Q、K、V线性映射\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        \n",
    "        # 拆分多头并计算注意力\n",
    "        q_split = self._split_heads(q)\n",
    "        k_split = self._split_heads(k)\n",
    "        v_split = self._split_heads(v)\n",
    "        \n",
    "        # 展平批次与头维度，适配SelfAttention输入\n",
    "        q_reshaped = q_split.view(-1, seq_len, self.head_dim)\n",
    "        k_reshaped = k_split.view(-1, seq_len, self.head_dim)\n",
    "        v_reshaped = v_split.view(-1, seq_len, self.head_dim)\n",
    "        mask_reshaped = mask.repeat(self.num_heads, 1, 1) if mask is not None else None\n",
    "        \n",
    "        att_split, att_weights_split = self.self_attn(q_reshaped, mask=mask_reshaped)\n",
    "        \n",
    "        # 拼接多头结果并线性变换\n",
    "        att_reshaped = att_split.view(batch_size, self.num_heads, seq_len, self.head_dim)\n",
    "        att_concat = self._concat_heads(att_reshaped)\n",
    "        out = self.linear_out(att_concat)\n",
    "        \n",
    "        # 计算所有头的平均注意力权重\n",
    "        att_weights = att_weights_split.view(batch_size, self.num_heads, seq_len, seq_len).mean(dim=1)\n",
    "        return out, att_weights\n",
    "\n",
    "\n",
    "# 实现Add&Norm层（Transformer Encoder核心组件）\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"Add&Norm层：通过残差连接缓解梯度消失，通过层归一化稳定训练\"\"\"\n",
    "    def __init__(self, dim_model, eps=1e-6):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(dim_model, eps=eps)  # 层归一化（对特征维度归一化）\n",
    "        # 残差连接权重：增强灵活性，初始为纯残差\n",
    "        self.residual_weight = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 当前模块输出（如注意力层输出），形状[batch_size, seq_len, dim_model]\n",
    "            residual: 残差输入（模块原始输入），形状与x一致\n",
    "        Returns:\n",
    "            out: Add&Norm后输出，形状与x一致\n",
    "        \"\"\"\n",
    "        # 残差相加（Add）\n",
    "        add_out = x + self.residual_weight * residual\n",
    "        # 层归一化（Norm）\n",
    "        out = self.norm(add_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 实现Feed Forward层（Transformer Encoder核心组件）\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"前馈网络：对注意力输出做非线性变换，增强模型表达能力\"\"\"\n",
    "    def __init__(self, dim_model, hidden_dim=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_model, hidden_dim, bias=True)  # 特征升维\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout正则化，防止过拟合\n",
    "        self.linear2 = nn.Linear(hidden_dim, dim_model, bias=True)  # 特征降维回原维度\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 输入张量，形状[batch_size, seq_len, dim_model]\n",
    "        Returns:\n",
    "            out: 前馈网络输出，形状与x一致\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear2(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 实现单个Encoder Layer（Transformer Encoder的基础单元）\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"单个Encoder Layer：注意力层→Add&Norm→前馈网络→Add&Norm的经典结构\"\"\"\n",
    "    def __init__(self, dim_model, num_heads, hidden_dim=2048, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attn = MultiHeadAttention(dim_model=dim_model, num_heads=num_heads)\n",
    "        self.add_norm1 = AddNorm(dim_model=dim_model)\n",
    "        self.feed_forward = FeedForward(dim_model=dim_model, hidden_dim=hidden_dim, dropout=dropout)\n",
    "        self.add_norm2 = AddNorm(dim_model=dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 输入张量，形状[batch_size, seq_len, dim_model]\n",
    "            mask: 可选掩码，形状[batch_size, seq_len, seq_len]\n",
    "        Returns:\n",
    "            out: 单个Encoder Layer输出，形状与x一致\n",
    "            att_weights: 注意力权重，形状[batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # 注意力层 + 第一次Add&Norm\n",
    "        att_out, att_weights = self.multi_head_attn(x, mask=mask)\n",
    "        att_out = self.dropout(att_out)\n",
    "        add_norm1_out = self.add_norm1(att_out, residual=x)\n",
    "        \n",
    "        # 前馈网络 + 第二次Add&Norm\n",
    "        ff_out = self.feed_forward(add_norm1_out)\n",
    "        ff_out = self.dropout(ff_out)\n",
    "        out = self.add_norm2(ff_out, residual=add_norm1_out)\n",
    "        \n",
    "        return out, att_weights\n",
    "\n",
    "\n",
    "# 实现完整Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder：堆叠多个Encoder Layer，实现序列的深度特征编码\"\"\"\n",
    "    def __init__(self, dim_model=512, num_heads=8, num_layers=6, hidden_dim=2048, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # 堆叠指定数量的Encoder Layer\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                dim_model=dim_model,\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 输入序列张量，形状[batch_size, seq_len, dim_model]\n",
    "            mask: 可选掩码，形状[batch_size, seq_len, seq_len]\n",
    "        Returns:\n",
    "            out: Encoder最终输出，形状[batch_size, seq_len, dim_model]\n",
    "            all_att_weights: 所有层的注意力权重，形状[num_layers, batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, dim_model = x.shape\n",
    "        assert dim_model == self.dim_model, f\"输入维度{dim_model}与Encoder dim_model{self.dim_model}不一致\"\n",
    "        \n",
    "        out = x\n",
    "        all_att_weights = []  # 记录所有层的注意力权重，便于后续分析\n",
    "        \n",
    "        # 逐层传递计算\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            out, att_weights = encoder_layer(out, mask=mask)\n",
    "            all_att_weights.append(att_weights)\n",
    "        \n",
    "        # 整理注意力权重形状\n",
    "        all_att_weights = torch.stack(all_att_weights, dim=0)\n",
    "        return out, all_att_weights\n",
    "\n",
    "\n",
    "# 测试Transformer Encoder功能\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 测试参数配置（参考Transformer常规设置）\n",
    "    batch_size = 2       # 批次大小\n",
    "    seq_len = 16         # 序列长度\n",
    "    dim_model = 512      # 输入特征维度\n",
    "    num_heads = 8        # 注意力头数（确保dim_model能被整除）\n",
    "    num_layers = 6       # Encoder堆叠层数\n",
    "    \n",
    "    # 2. 生成随机输入张量\n",
    "    x = torch.randn(batch_size, seq_len, dim_model)  # [2, 16, 512]\n",
    "    \n",
    "    # 3. 生成掩码（屏蔽序列后8个无效位置）\n",
    "    mask = torch.ones(batch_size, seq_len, seq_len)  # [2, 16, 16]\n",
    "    mask[:, :, 8:] = 0\n",
    "    \n",
    "    # 4. 初始化Encoder并执行前向传播\n",
    "    encoder = TransformerEncoder(\n",
    "        dim_model=dim_model,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers\n",
    "    )\n",
    "    encoder_out, encoder_att_weights = encoder(x, mask=mask)\n",
    "    \n",
    "    # 5. 验证输出结果\n",
    "    print(\"=== Transformer Encoder测试结果 ===\")\n",
    "    print(f\"输入随机矩阵形状：{x.shape}\")\n",
    "    print(f\"Encoder输出形状：{encoder_out.shape}（预期：[{batch_size}, {seq_len}, {dim_model}]）\")\n",
    "    print(f\"注意力权重形状：{encoder_att_weights.shape}（预期：[{num_layers}, {batch_size}, {seq_len}, {seq_len}]）\")\n",
    "    print(f\"掩码有效性：第1层第1批次第1序列后8位权重均值 = {encoder_att_weights[0, 0, 0, 8:].mean():.6f}（接近0为正常）\")\n",
    "    print(\"\\nTransformer Encoder实现完成，可正常用于后续任务！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
