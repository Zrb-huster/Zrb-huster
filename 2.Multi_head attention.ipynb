{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d533190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Head Attention测试结果 ===\n",
      "输入形状：torch.Size([2, 16, 512])\n",
      "输出形状：torch.Size([2, 16, 512])（预期：[2, 16, 512]）\n",
      "注意力权重形状：torch.Size([2, 16, 16])（预期：[2, 16, 16]）\n",
      "掩码有效性：第一批次第一序列后8位权重均值 = 0.000000（接近0为正常）\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_q, dim_k, dim_v):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.linear_q = nn.Linear(dim_q, dim_k, bias=False)\n",
    "        self.linear_k = nn.Linear(dim_q, dim_k, bias=False)\n",
    "        self.linear_v = nn.Linear(dim_q, dim_v, bias=False)\n",
    "        self._norm_fact = 1 / math.sqrt(dim_k)  # 缩放因子，避免分数值过大\n",
    "        self.dim_q = dim_q\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, dim_q = x.shape\n",
    "        assert dim_q == self.dim_q, \"输入维度与初始化dim_q不匹配\"\n",
    "        \n",
    "        # 计算Q、K、V\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        \n",
    "        # 注意力分数计算与缩放\n",
    "        attn_scores = torch.bmm(q, k.transpose(1, 2)) * self._norm_fact\n",
    "        # 应用掩码（若有），掩码位置设为极小值以屏蔽注意力\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 注意力权重归一化与上下文向量计算\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        att = torch.bmm(attn_weights, v)\n",
    "        \n",
    "        return att, attn_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert dim_model % num_heads == 0, f\"输入维度{dim_model}需能被头数{num_heads}整除\"\n",
    "        \n",
    "        self.dim_model = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim_model // num_heads  # 每个头的维度\n",
    "        \n",
    "        # Q/K/V线性映射层\n",
    "        self.linear_q = nn.Linear(dim_model, dim_model, bias=False)\n",
    "        self.linear_k = nn.Linear(dim_model, dim_model, bias=False)\n",
    "        self.linear_v = nn.Linear(dim_model, dim_model, bias=False)\n",
    "        \n",
    "        # 复用自注意力模块\n",
    "        self.self_attn = SelfAttention(\n",
    "            dim_q=self.head_dim, \n",
    "            dim_k=self.head_dim, \n",
    "            dim_v=self.head_dim\n",
    "        )\n",
    "        \n",
    "        # 多头拼接后的输出线性层\n",
    "        self.linear_out = nn.Linear(dim_model, dim_model, bias=False)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"将输入拆分为多个头\n",
    "        输入x：[batch_size, seq_len, dim_model]\n",
    "        输出：[batch_size, num_heads, seq_len, head_dim]\n",
    "        注：transpose后调用contiguous()确保张量内存连续，避免view报错\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, dim_model = x.shape\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def _concat_heads(self, x):\n",
    "        \"\"\"将多个头的输出拼接为整体\n",
    "        输入x：[batch_size, num_heads, seq_len, head_dim]\n",
    "        输出：[batch_size, seq_len, dim_model]\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, num_heads * head_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, dim_model = x.shape\n",
    "        assert dim_model == self.dim_model, \"输入维度与初始化dim_model不匹配\"\n",
    "        \n",
    "        # 1. Q/K/V线性映射\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        \n",
    "        # 2. 拆分多头\n",
    "        q_split = self._split_heads(q)\n",
    "        k_split = self._split_heads(k)\n",
    "        v_split = self._split_heads(v)\n",
    "        \n",
    "        # 3. 子空间自注意力计算（展平batch+heads维度以适配SelfAttention）\n",
    "        q_reshaped = q_split.view(-1, seq_len, self.head_dim)\n",
    "        k_reshaped = k_split.view(-1, seq_len, self.head_dim)\n",
    "        v_reshaped = v_split.view(-1, seq_len, self.head_dim)\n",
    "        mask_reshaped = mask.repeat(self.num_heads, 1, 1) if mask is not None else None\n",
    "        \n",
    "        att_split, att_weights_split = self.self_attn(q_reshaped, mask=mask_reshaped)\n",
    "        \n",
    "        # 4. 多头拼接与输出线性变换\n",
    "        att_reshaped = att_split.view(batch_size, self.num_heads, seq_len, self.head_dim)\n",
    "        att_concat = self._concat_heads(att_reshaped)\n",
    "        out = self.linear_out(att_concat)\n",
    "        \n",
    "        # 计算所有头的平均注意力权重\n",
    "        att_weights = att_weights_split.view(batch_size, self.num_heads, seq_len, seq_len).mean(dim=1)\n",
    "        \n",
    "        return out, att_weights\n",
    "\n",
    "\n",
    "# 测试模块功能\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    seq_len = 16\n",
    "    dim_model = 512\n",
    "    num_heads = 8\n",
    "    \n",
    "    # 随机生成输入张量\n",
    "    x = torch.randn(batch_size, seq_len, dim_model)\n",
    "    # 生成掩码（屏蔽序列后8位）\n",
    "    mask = torch.ones(batch_size, seq_len, seq_len)\n",
    "    mask[:, :, 8:] = 0\n",
    "    \n",
    "    # 初始化Multi-Head Attention并执行前向传播\n",
    "    multi_head_attn = MultiHeadAttention(dim_model=dim_model, num_heads=num_heads)\n",
    "    mh_output, mh_att_weights = multi_head_attn(x, mask=mask)\n",
    "    \n",
    "    # 验证输出结果\n",
    "    print(\"=== Multi-Head Attention测试结果 ===\")\n",
    "    print(f\"输入形状：{x.shape}\")\n",
    "    print(f\"输出形状：{mh_output.shape}（预期：[{batch_size}, {seq_len}, {dim_model}]）\")\n",
    "    print(f\"注意力权重形状：{mh_att_weights.shape}（预期：[{batch_size}, {seq_len}, {seq_len}]）\")\n",
    "    print(f\"掩码有效性：第一批次第一序列后8位权重均值 = {mh_att_weights[0, 0, 8:].mean():.6f}（接近0为正常）\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
